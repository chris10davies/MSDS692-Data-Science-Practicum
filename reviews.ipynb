{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "\n",
    "\n",
    "# Text Processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import wordcloud\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.collocations as nc\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Remove stopwords, including google and tesla\n",
    "stops = stopwords.words('english') + ['google', 'tesla', 'work']\n",
    "en_stopwords = set(stops)\n",
    "from textblob import TextBlob\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# Time Series\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "# Geo\n",
    "import folium\n",
    "import os\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from pyclustering.cluster.silhouette import silhouette_ksearch_type, silhouette_ksearch\n",
    "import operator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Datasets \n",
    "#### (Google & Tesla reviews web-scraped from Indeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets - web scraped data\n",
    "\n",
    "# Code Source \n",
    "# https://stackoverflow.com/questions/16888888/how-to-read-a-xlsx-file-\n",
    "# using-the-pandas-library-in-ipython\n",
    "\n",
    "df_google = pd.read_excel('google_reviews.xlsx', sheet_name=\"sheet1\")\n",
    "df_tesla = pd.read_excel('tesla_reviews.xlsx', sheet_name=\"sheet1\")\n",
    "\n",
    "# Import state data for geo\n",
    "\n",
    "# Code Source \n",
    "# https://developers.google.com/public-data/docs/canonical/states_csv\n",
    "df_state_geo = pd.read_csv('states_geo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some information about the datasets\n",
    "\n",
    "# Code Source\n",
    "# https://www.datacamp.com/community/tutorials/time-series-analysis-tutorial\n",
    "\n",
    "df_google.info()\n",
    "df_tesla.info()\n",
    "\n",
    "# No missing values - all objects, one float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Convert data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Source\n",
    "# https://www.datacamp.com/community/tutorials/time-series-analysis-tutorial\n",
    "\n",
    "# Date to date data type\n",
    "df_google.date = pd.to_datetime(df_google.date)\n",
    "df_tesla.date = pd.to_datetime(df_tesla.date)\n",
    "\n",
    "# Rating to integer\n",
    "df_google.rating = df_google.rating.astype(np.int64)\n",
    "df_tesla.rating = df_tesla.rating.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the google dataframe\n",
    "df_google.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the tesla dataframe\n",
    "df_tesla.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Create employee type (former or current employee) from reviewer variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip out former/current employee and make it's own variable\n",
    "\n",
    "# Code Source: \n",
    "# https://www.geeksforgeeks.org/python-pandas-split-strings-into-two-list-columns-using-str-split/\n",
    "\n",
    "# split reviewer variable on '(' \n",
    "new_google = df_google[\"reviewer\"].str.split(\"(\", n = 1, expand = True) \n",
    "new_tesla = df_tesla[\"reviewer\"].str.split(\"(\", n = 1, expand = True) \n",
    "\n",
    "# make job title column from the left part of the split \n",
    "df_google[\"job_title\"]= new_google[0] \n",
    "df_tesla[\"job_title\"]= new_tesla[0] \n",
    "  \n",
    "# make emp type column from the right part of the split\n",
    "df_google[\"emp_status\"]= new_google[1] \n",
    "df_tesla[\"emp_status\"]= new_tesla[1] \n",
    "\n",
    "# Drop reviewer column \n",
    "df_google.drop('reviewer', inplace=True, axis=1)\n",
    "df_tesla.drop('reviewer', inplace=True, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now have 7 variables, created employee type & job title, dropped reviewer\n",
    "df_google.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tesla.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count of each employee status category\n",
    "\n",
    "# Code Source\n",
    "# https://stackoverflow.com/questions/22391433/count-the-frequency\n",
    "# -that-a-value-occurs-in-a-dataframe-column\n",
    "\n",
    "df_google['emp_status'].value_counts()\n",
    "\n",
    "# Lots of junk in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tesla['emp_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty much an if statement - if emp_status contains 'Current', make it 'current', else make it 'former'\n",
    "\n",
    "# Code Source\n",
    "# https://stackoverflow.com/questions/18196203/\n",
    "# how-to-conditionally-update-dataframe-column-in-pandas\n",
    "\n",
    "df_google['emp_status'] = np.where(df_google['emp_status'].str.contains('Current'), 'current', 'former')\n",
    "\n",
    "df_tesla['emp_status'] = np.where(df_tesla['emp_status'].str.contains('Current'), 'current', 'former')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean categories - current/former\n",
    "df_google['emp_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tesla['emp_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create state from location variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip out state and make it's own variable\n",
    "\n",
    "# split location variable on ',' \n",
    "new_google = df_google[\"location\"].str.split(\",\", n = 1, expand = True) \n",
    "new_tesla = df_tesla[\"location\"].str.split(\",\", n = 1, expand = True) \n",
    "\n",
    "# make state column from the right part of the split \n",
    "df_google[\"state\"]= new_google[1] \n",
    "df_tesla[\"state\"]= new_tesla[1] \n",
    "\n",
    "# Drop location column \n",
    "df_google.drop('location', inplace=True, axis=1)\n",
    "df_tesla.drop('location', inplace=True, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first two characters\n",
    "\n",
    "# Code Source\n",
    "# https://stackoverflow.com/questions/36505847/substring-of-an-entire-column-in-pandas-dataframe\n",
    "\n",
    "df_google.state = df_google.state.str.slice(1, 3)\n",
    "df_tesla.state = df_tesla.state.str.slice(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper case the state code extracted\n",
    "\n",
    "# Code Source\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.upper.html\n",
    "\n",
    "df_google.state = df_google.state.str.upper()\n",
    "df_tesla.state = df_tesla.state.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count of each employee status category\n",
    "df_google['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate states extracted against state list,\n",
    "# Not found assign ERR\n",
    "\n",
    "# Code Source\n",
    "# https://thispointer.com/python-how-to-check-if-an-item-exists-in-list-search-by-value-or-condition/\n",
    "\n",
    "def state_lookup(df):\n",
    "    # If valid state code, keep it, else assign 'ERR'\n",
    "    for i in range(0,len(df)):\n",
    "        if df.iloc[i] in df_state_geo['state'].values.tolist():\n",
    "            df[i] = df[i]\n",
    "        else:\n",
    "            df[i] = 'ERR'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_lookup(df_google.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_lookup(df_tesla.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count of reviews in each state\n",
    "df_google['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a count of reviews in each state\n",
    "df_tesla['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge longitude/latitude to dataframe, may use later for geo\n",
    "\n",
    "# Code source\n",
    "# https://medium.com/importexcel/common-excel-task-\n",
    "# in-python-vlookup-with-pandas-merge-c99d4e108988\n",
    "\n",
    "df_google = pd.merge(df_google,df_state_geo, how='left', on='state')\n",
    "df_tesla = pd.merge(df_tesla, df_state_geo, how='left', on='state')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign company variable\n",
    "df_google['Company'] = 'google'\n",
    "df_tesla['Company'] = 'tesla'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union google and tesla datasets for bar chart\n",
    "\n",
    "# Code Source\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "\n",
    "frames = [df_google, df_tesla]\n",
    "\n",
    "df_comp = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data type for categorical variables\n",
    "\n",
    "# Code source\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n",
    "\n",
    "df_comp['emp_status'] = df_comp['emp_status'].astype('category')\n",
    "df_comp['Company'] = df_comp['Company'].astype('category')\n",
    "df_comp['state'] = df_comp['state'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore newly created employee status variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot - Percentage of reviews by current/former empployees\n",
    "\n",
    "# Code sournce\n",
    "# https://stackoverflow.com/questions/35692781/python-plotting-percentage-in-seaborn-bar-plot\n",
    "\n",
    "x, y, hue = \"Company\", \"proportion\", \"emp_status\"\n",
    "# hue_order = [\"Male\", \"Female\"]\n",
    "\n",
    "(df_comp[x]\n",
    " .groupby(df_comp[hue])\n",
    " .value_counts()\n",
    " .rename(y)\n",
    " .reset_index()\n",
    " .pipe((sns.barplot, \"data\"), x=x, y=y, hue=hue));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore rating variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Rating Summary for Google\n",
    "df_google['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating Frequency Bar Plot Google\n",
    "df_google['rating'].value_counts().plot.bar(color='Gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Rating Summary for Tesla\n",
    "df_tesla['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating Frequency Bar Plot Google\n",
    "df_tesla['rating'].value_counts().plot.bar(color='Gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore state variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Frequency Bar Plot Google\n",
    "df_google['state'].value_counts().plot.bar(color='Gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Frequency Bar Plot Google\n",
    "df_tesla['state'].value_counts().plot.bar(color='Gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and reviews to clean for each review\n",
    "# Trying to not combine -  many titles were duped in review\n",
    "# google_review_text = df_google[\"title\"] + ' ' + df_google[\"reviews\"]\n",
    "# tesla_review_text = df_tesla[\"title\"] + ' ' + df_tesla[\"reviews\"]\n",
    "\n",
    "google_review_text = df_google[\"reviews\"]\n",
    "tesla_review_text = df_tesla[\"reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a closer look at combine review text\n",
    "google_review_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_review_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "\n",
    "# Parts of code from Week 3 and Week 6 Solutions\n",
    "# Regis Text Analytics Class - Dr. Nathan George\n",
    "\n",
    "def preprocess_text(text_docs):\n",
    "   \n",
    "    # remove punctuation\n",
    "    table = str.maketrans({key: None for key in string.punctuation})\n",
    "    docs = [str(d).translate(table) for d in text_docs]\n",
    "    # replace newlines with spaces\n",
    "    docs = [re.sub('[\\r\\n]+', ' ', d) for d in docs]\n",
    "    # replace website links with space -- want to do this before digits\n",
    "    docs = [re.sub('https[\\w]*', ' ', d) for d in docs]\n",
    "    # replace digits with space\n",
    "    docs = [re.sub('\\d', ' ', d) for d in docs]\n",
    "    # relpace multiple spaces with one\n",
    "    docs = [re.sub('\\s\\s+', ' ', d) for d in docs]\n",
    "    \n",
    "    \n",
    "    # process with spacy\n",
    "    spacy_docs = [nlp(d) for d in docs]\n",
    "    lemmatized_docs = []\n",
    "    \n",
    "    # keep the word if it's a pronoun, otherwise use the lemma\n",
    "    lemmas = [[w.lemma_ if w.lemma_ != '-PRON-'\n",
    "                   else w.lower_\n",
    "                   for w in d if w.lower_ not in en_stopwords]\n",
    "              for d in spacy_docs]\n",
    "    \n",
    "    \n",
    "    pos = [[w.pos_ for w in d if w.lower_ not in en_stopwords] for d in spacy_docs]\n",
    "    flat_lemmas = [i for l in lemmas for i in l]\n",
    "    flat_pos = [i for p in pos for i in p]\n",
    "    df = pd.DataFrame({'word': flat_lemmas, 'pos': flat_pos})\n",
    "    \n",
    "    lemmatized_docs = [' '.join(l) for l in lemmas]\n",
    "    \n",
    "    return lemmatized_docs, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocess_text for reviews\n",
    "cln_review_google, df_pos_google = preprocess_text(google_review_text)\n",
    "cln_review_tesla, df_pos_tesla = preprocess_text(tesla_review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a closer look at cleaned review\n",
    "cln_review_google[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cln_review_tesla[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of grouped words - part of speech, word\n",
    "\n",
    "# Code Source\n",
    "# From Text Analytics Week 3 - Dr. Nathan George\n",
    "\n",
    "words_google = list(df_pos_google.groupby('word'))\n",
    "words_tesla = list(df_pos_tesla.groupby('word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets value counts of words - word, pos, count\n",
    "\n",
    "# From Text Analytics Week 3 - Dr. Nathan George\n",
    "\n",
    "all_cnts_google = {}\n",
    "for w, w_df in words_google:\n",
    "    all_cnts_google[w] = w_df['pos'].value_counts()\n",
    "    \n",
    "all_cnts_tesla = {}\n",
    "for w, w_df in words_tesla:\n",
    "    all_cnts_tesla[w] = w_df['pos'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total up nouns and verbs\n",
    "\n",
    "# From Text Analytics Week 3 - Dr. Nathan George\n",
    "\n",
    "nouns_google, verbs_google = {}, {}\n",
    "for w in all_cnts_google:\n",
    "    if 'NOUN' in all_cnts_google[w].keys():\n",
    "        nouns_google[w] = all_cnts_google[w]['NOUN']\n",
    "    \n",
    "    if 'VERB' in all_cnts_google[w].keys():\n",
    "        verbs_google[w] = all_cnts_google[w]['VERB']\n",
    "        \n",
    "        \n",
    "nouns_tesla, verbs_tesla = {}, {}\n",
    "for w in all_cnts_tesla:\n",
    "    if 'NOUN' in all_cnts_tesla[w].keys():\n",
    "        nouns_tesla[w] = all_cnts_tesla[w]['NOUN']\n",
    "    \n",
    "    if 'VERB' in all_cnts_tesla[w].keys():\n",
    "        verbs_tesla[w] = all_cnts_tesla[w]['VERB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report out top 10 nouns and verbs per company\n",
    "\n",
    "# Code Source from Week 3 Text Analytics - Dr. Nathan George\n",
    "# https://stackoverflow.com/questions/613183/how-to-sort-a-dictionary-by-value\n",
    "\n",
    "# Sorting most nouns/verbs to the least\n",
    "top_nouns_google = sorted(nouns_google, key=nouns_google.get, reverse=True)\n",
    "top_noun_counts_google = [nouns_google[n] for n in top_nouns_google]\n",
    "top_verbs_google = sorted(verbs_google, key=verbs_google.get, reverse=True)\n",
    "top_verb_counts_google = [verbs_google[v] for v in top_verbs_google]\n",
    "\n",
    "top_nouns_tesla = sorted(nouns_tesla, key=nouns_tesla.get, reverse=True)\n",
    "top_noun_counts_tesla = [nouns_tesla[n] for n in top_nouns_tesla]\n",
    "top_verbs_tesla = sorted(verbs_tesla, key=verbs_tesla.get, reverse=True)\n",
    "top_verb_counts_tesla = [verbs_tesla[v] for v in top_verbs_tesla]\n",
    "\n",
    "print('top 10 Google nouns:\\n')\n",
    "for n, c in zip(top_nouns_google[:10], top_noun_counts_google[:10]):\n",
    "    print(n, '(' + str(c) + ' times)')\n",
    "    \n",
    "print('\\ntop 10 Google verbs:\\n')\n",
    "for n, c in zip(top_verbs_google[:10], top_verb_counts_google[:10]):\n",
    "    print(n, '(' + str(c) + ' times)')\n",
    "    \n",
    "print('\\ntop 10 Tesla nouns:\\n')\n",
    "for n, c in zip(top_nouns_tesla[:10], top_noun_counts_tesla[:10]):\n",
    "    print(n, '(' + str(c) + ' times)')\n",
    "    \n",
    "print('\\ntop 10 Tesla verbs:\\n')\n",
    "for n, c in zip(top_verbs_tesla[:10], top_verb_counts_tesla[:10]):\n",
    "    print(n, '(' + str(c) + ' times)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top n-grams from the reviews\n",
    "\n",
    "# From Text Analytics Week 3 - Dr. Nathan George\n",
    "\n",
    "def get_top_grams_google(docs, n=2, top=10):\n",
    "    \n",
    "    v_google = CountVectorizer(ngram_range=(n, n))\n",
    "    grams_google = v_google.fit_transform(docs)\n",
    "    # convert to array and flatten to avoid weird indexing\n",
    "    gram_sum_google = np.array(np.sum(grams_google, axis=0)).flatten()\n",
    "    gram_dict_google = {i: v for v, i in v_google.vocabulary_.items()}  # dictionary of index: word\n",
    "    top_grams_google = gram_sum_google.argsort()[::-1]\n",
    "    for i in top_grams_google[:top]:\n",
    "        print('\"' + gram_dict_google[i] + '\" shows up', gram_sum_google[i], 'times')\n",
    "    \n",
    "    return [gram_dict_google[i] for i in top_grams_google], gram_sum_google[top_grams_google]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report out top 10 for 1,2,3, and 4 grams - google\n",
    "\n",
    "# From Text Analytics Week 3 - Dr. Nathan George\n",
    "\n",
    "ngrams_google, ngram_counts_google = {}, {}\n",
    "for n in [1, 2, 3, 4]:\n",
    "    print('top 10', str(n) + '-grams:\\n')\n",
    "    ngrams_google[n], ngram_counts_google[n] = get_top_grams_google(cln_review_google, n=n)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_grams_tesla(docs, n=2, top=10):\n",
    "    \n",
    "    v_tesla = CountVectorizer(ngram_range=(n, n))\n",
    "    grams_tesla = v_tesla.fit_transform(docs)\n",
    "    # convert to array and flatten to avoid weird indexing\n",
    "    gram_sum_tesla = np.array(np.sum(grams_tesla, axis=0)).flatten()\n",
    "    gram_dict_tesla = {i: v for v, i in v_tesla.vocabulary_.items()}  # dictionary of index: word\n",
    "    top_grams_tesla = gram_sum_tesla.argsort()[::-1]\n",
    "    for i in top_grams_tesla[:top]:\n",
    "        print('\"' + gram_dict_tesla[i] + '\" shows up', gram_sum_tesla[i], 'times')\n",
    "    \n",
    "    return [gram_dict_tesla[i] for i in top_grams_tesla], gram_sum_tesla[top_grams_tesla]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report out top 10 for 1, 2,3, and 4 grams - tesla\n",
    "\n",
    "ngrams_tesla, ngram_counts_tesla = {}, {}\n",
    "for n in [1, 2, 3, 4]:\n",
    "    print('top 10', str(n) + '-grams:\\n')\n",
    "    ngrams_tesla[n], ngram_counts_tesla[n] = get_top_grams_tesla(cln_review_tesla, n=n)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF - fit/transform\n",
    "\n",
    "# min_df=3 ignores terms occuring in less than 3 documents\n",
    "\n",
    "#Code from Week 4 Solution - Regis Text Analytics Class - Dr. Nathan George\n",
    "\n",
    "# Instantiate tfidf vectorizer and fit_transform\n",
    "tfidf_vectorizer_google = TfidfVectorizer(min_df=3,ngram_range=(1,1))\n",
    "train_tfidf_vectors_google = tfidf_vectorizer_google.fit_transform(cln_review_google)\n",
    "\n",
    "tfidf_vectorizer_tesla = TfidfVectorizer(min_df=3,ngram_range=(1,1))\n",
    "train_tfidf_vectors_tesla = tfidf_vectorizer_tesla.fit_transform(cln_review_tesla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_vectors_google.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_vectors_tesla.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate wordcloud and set size\n",
    "wc = wordcloud.WordCloud(width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for 1,2,3,4 grams - google\n",
    "\n",
    "# Week 3 Text Analytics - Dr. Nathan George\n",
    "\n",
    "for n in [1, 2, 3, 4]:\n",
    "    f = wc.generate_from_frequencies(frequencies={w: i for w, i in zip(ngrams_google[n], ngram_counts_google[n])})\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    _ = plt.imshow(f, interpolation='bilinear')\n",
    "    _ = plt.axis(\"off\")  # assign result to a dummy variable so it doesn't show anything\n",
    "    _ = plt.title(str(n) + '-grams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for 1,2,3,4 grams - tesla\n",
    "\n",
    "for n in [1, 2, 3, 4]:\n",
    "    f = wc.generate_from_frequencies(frequencies={w: i for w, i in zip(ngrams_tesla[n], ngram_counts_tesla[n])})\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    _ = plt.imshow(f, interpolation='bilinear')\n",
    "    _ = plt.axis(\"off\")  # assign result to a dummy variable so it doesn't show anything\n",
    "    _ = plt.title(str(n) + '-grams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts by date\n",
    "\n",
    "# Code Source\n",
    "# https://stackoverflow.com/questions/22391433/\n",
    "# count-the-frequency-that-a-value-occurs-in-a-dataframe-column\n",
    "\n",
    "df_date_count_google = df_google['date'].value_counts()\n",
    "df_date_count_tesla = df_tesla['date'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a dataframe\n",
    "df_date_google = pd.DataFrame(df_date_count_google)\n",
    "df_date_tesla = pd.DataFrame(df_date_count_tesla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_google.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting into format for time series plot\n",
    "# reset and name index, rename columns\n",
    "\n",
    "# Code Source\n",
    "# https://stackoverflow.com/questions/26097916/\n",
    "# convert-pandas-series-to-dataframe\n",
    "\n",
    "df_count_google = df_date_google.date.to_frame().reset_index()\n",
    "df_count_google = df_count_google.rename(columns= {'index': 'date', 'date': 'count' })\n",
    "df_count_google.index.name = 'index'\n",
    "\n",
    "df_count_tesla = df_date_tesla.date.to_frame().reset_index()\n",
    "df_count_tesla = df_count_tesla.rename(columns= {'index': 'date', 'date': 'count' })\n",
    "df_count_tesla.index.name = 'index'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_google.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set date as the index for time series plots\n",
    "\n",
    "# Code Source - # https://www.datacamp.com/community/tutorials/time-series-analysis-tutorial\n",
    "\n",
    "df_count_google.set_index('date', inplace=True)\n",
    "df_count_tesla.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_google.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Source\n",
    "# https://stackoverflow.com/questions/38792122/how-to-group-and-count-rows-by-month-and-year-using-pandas\n",
    "\n",
    "df_count_google = df_count_google.resample('MS').size()\n",
    "df_count_tesla = df_count_tesla.resample('MS').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot time series google\n",
    "\n",
    "# Code Source - \n",
    "# https://www.datacamp.com/community/tutorials/time-series-analysis-tutorial\n",
    "\n",
    "df_count_google.plot(figsize=(20,10),linewidth = 5,fontsize=20)\n",
    "plt.xlabel('Year',fontsize=20);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series tesla\n",
    "df_count_tesla.plot(figsize=(20,10),linewidth = 5,fontsize=20)\n",
    "plt.xlabel('Year',fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series rolling mean google\n",
    "\n",
    "# Code Source - \n",
    "# https://www.datacamp.com/community/tutorials/time-series-analysis-tutorial\n",
    "\n",
    "df_count_google.rolling(12).mean().plot(figsize=(20,10), linewidth=5, fontsize=20)\n",
    "plt.xlabel('Year', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series rolling mean tesla\n",
    "\n",
    "df_count_tesla.rolling(12).mean().plot(figsize=(20,10), linewidth=5, fontsize=20)\n",
    "plt.xlabel('Year', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at day of week reviews were submitted\n",
    "\n",
    "# Covert to day of the week (Monday=0, Sunday=6)\n",
    "\n",
    "# Code Source\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.dayofweek.html\n",
    "\n",
    "df_google_day_of_week = df_comp['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case statement to match integer to day of week text\n",
    "\n",
    "# Code Source\n",
    "# https://data-flair.training/blogs/python-switch-case/\n",
    "\n",
    "def week(i):\n",
    "    switcher={\n",
    "        0:'Monday',\n",
    "        1:'Tuesday',\n",
    "        2:'Wednesday',\n",
    "        3:'Thursday',\n",
    "        4:'Friday',\n",
    "        5:'Saturday',\n",
    "        6:'Sunday'\n",
    "             }\n",
    "    return switcher.get(i,\"Invalid day of week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating file to plot day of week by company comparison\n",
    "\n",
    "# Call function - add new variable to df_comp\n",
    "df_comp['dayofweek'] = np.array([week(i) for i in df_google_day_of_week[0:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data type for categorical variables\n",
    "df_comp['dayofweek'] = df_comp['dayofweek'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot - review day of week by company\n",
    "\n",
    "# Code Source\n",
    "# https://stackoverflow.com/questions/35692781/python-plotting-percentage-in-seaborn-bar-plot\n",
    "\n",
    "x, y, hue = \"dayofweek\", \"proportion\", \"Company\"\n",
    "\n",
    "(df_comp[x]\n",
    " .groupby(df_comp[hue])\n",
    " .value_counts(normalize=True)\n",
    " .rename(y)\n",
    " .reset_index()\n",
    " .pipe((sns.barplot, \"data\",), x=x, y=y, hue=hue,));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab reviews variable\n",
    "\n",
    "# Code source:\n",
    "# https://dev.to/rodolfoferro/sentiment-analysis-on-trumpss-tweets-using-python-\n",
    "\n",
    "df_google_cln_rev = pd.DataFrame(data=cln_review_google, columns=['Reviews'])\n",
    "df_tesla_cln_rev = pd.DataFrame(data=cln_review_tesla, columns=['Reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using textblob for sentiment,\n",
    "# assign 1, 0, -1 based on polarity\n",
    "\n",
    "# Code source:\n",
    "# https://dev.to/rodolfoferro/sentiment-analysis-on-trumpss-tweets-using-python-\n",
    "\n",
    "def analize_sentiment(Reviews):\n",
    "   \n",
    "    analysis = TextBlob(Reviews)\n",
    "    if analysis.sentiment.polarity >= .3:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity < .3 and analysis.sentiment.polarity >= -.3 :\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function and add SA to dataframe\n",
    "\n",
    "# Code source\n",
    "# https://dev.to/rodolfoferro/sentiment-analysis-on-trumpss-tweets-using-python-\n",
    "\n",
    "df_google['SA'] = np.array([analize_sentiment(Reviews) for Reviews in df_google_cln_rev['Reviews']])\n",
    "df_tesla['SA'] = np.array([analize_sentiment(Reviews) for Reviews in df_tesla_cln_rev['Reviews']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out polarity to dataframe as well\n",
    "\n",
    "def analize_polarity(Reviews):\n",
    "   \n",
    "    analysis = TextBlob(Reviews)\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google['Polarity'] = np.array([analize_polarity(Reviews) for Reviews in df_google_cln_rev['Reviews']])\n",
    "df_tesla['Polarity'] = np.array([analize_polarity(Reviews) for Reviews in df_tesla_cln_rev['Reviews']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tesla.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at sentiment compared to rating\n",
    "\n",
    "# Code Source\n",
    "# https://pbpython.com/pandas-crosstab.html\n",
    "\n",
    "pd.crosstab(df_google.rating, df_google.SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_tesla.rating, df_tesla.SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some reviews with positive sentiment and give a rating of 1\n",
    "# https://stackoverflow.com/questions/17071871/select-rows-from-a-dataframe-based-on-values-in-a-column-in-pandas\n",
    "\n",
    "df_tesla[['reviews','Polarity']].loc[(df_tesla['rating'] == 1) & (df_tesla['SA'] == 1)].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out how textblob is determing sentiment - idea from Dr. George\n",
    "t = TextBlob('Same as every other tech company now. They will only pay the top people what they want then the rest are left to pick up the pieces. Tesla name carries weight and they know it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried to play with sentiment ranges above to improve accuracy, really need to train sentiment model\n",
    "t.sentiment_assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files with state/sentiment mean and state/rating mean\n",
    "\n",
    "# Code Source\n",
    "# https://towardsdatascience.com/pandas-tips-and-tricks-33bcc8a40bb9\n",
    "\n",
    "df_google_state_SA = df_google.groupby('state')['SA'].mean()\n",
    "df_tesla_state_SA = df_tesla.groupby('state')['SA'].mean()\n",
    "\n",
    "df_google_state_rating = df_google.groupby('state')['rating'].mean()\n",
    "df_tesla_state_rating = df_tesla.groupby('state')['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make them dataframes \n",
    "\n",
    "df_google_state_SA = pd.DataFrame(df_google_state_SA)\n",
    "df_tesla_state_SA = pd.DataFrame(df_tesla_state_SA)\n",
    "\n",
    "df_google_state_rating = pd.DataFrame(df_google_state_rating)\n",
    "df_tesla_state_rating = pd.DataFrame(df_tesla_state_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indexes\n",
    "\n",
    "df_google_state_SA.reset_index(inplace=True)\n",
    "df_tesla_state_SA.reset_index(inplace=True)\n",
    "\n",
    "df_google_state_rating.reset_index(inplace=True)\n",
    "df_tesla_state_rating.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the ERR states\n",
    "\n",
    "# https://stackoverflow.com/questions/34397982/pandas-dataframe-access-multiple-items-with-not-equal-to\n",
    "notERR_google_SA = df_google_state_SA[df_google_state_SA['state'] != 'ERR']\n",
    "notERR_tesla_SA = df_tesla_state_SA[df_tesla_state_SA['state'] != 'ERR']\n",
    "\n",
    "# https://stackoverflow.com/questions/34397982/pandas-dataframe-access-multiple-items-with-not-equal-to\n",
    "notERR_google_rating = df_google_state_rating[df_google_state_rating['state'] != 'ERR']\n",
    "notERR_tesla_rating = df_tesla_state_rating[df_tesla_state_rating['state'] != 'ERR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks good!\n",
    "notERR_google_SA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Folium choropleth of average sentiment by state - Google\n",
    "\n",
    "# Code sources\n",
    "# https://medium.com/@austinlasseter/using-folium-to-generate-a-simple-map-of-your-pandas-data-87ddc5d55f8d\n",
    "# https://python-graph-gallery.com/292-choropleth-map-with-folium/\n",
    "# https://github.com/bradtraversy/python_folium_example/edit/master/data/us-states.json\n",
    "\n",
    "map = folium.Map(location=[48, -102], zoom_start=3)\n",
    "\n",
    "#mac\n",
    "state_geo = os.path.join('/Users/christiandavies/Desktop/data_pract_1/Project', 'us-states.json')\n",
    "\n",
    "# windows \n",
    "# state_geo = os.path.join(r\"C:\\Users\\Czdavies\\Desktop\\DS_prac_1\\project\", 'us-states.json')\n",
    "\n",
    "map.choropleth(geo_data=state_geo, data=notERR_google_SA,\n",
    "             columns=['state', 'SA'],\n",
    "             key_on='feature.id',\n",
    "             fill_color='YlGn', fill_opacity=0.7, line_opacity=0.2,\n",
    "             legend_name='Sentiment')\n",
    "\n",
    "map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Folium choropleth of average rating by state - Google\n",
    "\n",
    "map = folium.Map(location=[48, -102], zoom_start=3)\n",
    "\n",
    "map.choropleth(geo_data=state_geo, data=notERR_google_rating,\n",
    "             columns=['state', 'rating'],\n",
    "             key_on='feature.id',\n",
    "             fill_color='YlGn', fill_opacity=0.7, line_opacity=0.2,\n",
    "             legend_name='Rating')\n",
    "\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Folium choropleth of average sentiment by state - Tesla\n",
    "\n",
    "\n",
    "map = folium.Map(location=[48, -102], zoom_start=3)\n",
    "\n",
    "map.choropleth(geo_data=state_geo, data=notERR_tesla_SA,\n",
    "             columns=['state', 'SA'],\n",
    "             key_on='feature.id',\n",
    "             fill_color='YlGn', fill_opacity=0.7, line_opacity=0.2,\n",
    "             legend_name='Sentiment')\n",
    "\n",
    "map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Folium choropleth of average rating by state - Tesla\n",
    "\n",
    "map = folium.Map(location=[48, -102], zoom_start=3)\n",
    "\n",
    "map.choropleth(geo_data=state_geo, data=notERR_tesla_rating,\n",
    "             columns=['state', 'rating'],\n",
    "             key_on='feature.id',\n",
    "             fill_color='YlGn', fill_opacity=0.7, line_opacity=0.2,\n",
    "             legend_name='Rating')\n",
    "\n",
    "map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning - Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can't use - not getting accurate optimal # of clusters, just going to use TF-IDF\n",
    "\n",
    "# Convert additional features for machine learning\n",
    "\n",
    "# df_google_emp_status = pd.get_dummies(df_google['emp_status'])\n",
    "# df_tesla_emp_status = pd.get_dummies(df_tesla['emp_status'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can't use - not getting accurate optimal # of clusters, just going to use TF-IDF\n",
    "\n",
    "# Convert tfidf to dataframe\n",
    "\n",
    "# Code source\n",
    "# https://stackoverflow.com/questions/36967666/transform-scipy-sparse-csr-to-pandas\n",
    "\n",
    "#df_tfidf_google = pd.DataFrame(train_tfidf_vectors_google.toarray())\n",
    "#df_tfidf_tesla = pd.DataFrame(train_tfidf_vectors_tesla.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can't use - not getting accurate optimal # of clusters, just going to use TF-IDF\n",
    "\n",
    "# Concatenate features to one dataframe\n",
    "\n",
    "#df_med_google = pd.concat([df_google.rating, df_google_emp_status, df_tfidf_google], axis=1, ignore_index=True) \n",
    "#df_med_google = df_med_google.rename(columns= {0: 'rating', 1: 'current', 2: 'former' })\n",
    "\n",
    "\n",
    "#df_med_tesla = pd.concat([df_tesla.rating, df_tesla_emp_status, df_tfidf_tesla], axis=1, ignore_index=True) \n",
    "#df_med_tesla = df_med_tesla.rename(columns= {0: 'rating', 1: 'current', 2: 'former'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to array for lsa\n",
    "google_array = np.asarray(df_med_google)\n",
    "tesla_array = np.asarray(df_med_tesla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA feature reduction - going with 100 components - google\n",
    "# Using feature reduction because pyclustering ran too long\n",
    "\n",
    "#Code source:\n",
    "# https://github.com/chrisjmccormick/LSA_Classification/blob/master/inspect_LSA.py\n",
    "# https://towardsdatascience.com/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547\n",
    "\n",
    "svd_google = TruncatedSVD(400)\n",
    "lsa_google = make_pipeline(svd_google, Normalizer(copy=False))\n",
    "x_train_lsa_google = lsa_google.fit_transform(train_tfidf_vectors_google)\n",
    "print('LSA output shape:', x_train_lsa_google.shape)\n",
    "explained_variance_google = svd_google.explained_variance_ratio_.sum()\n",
    "print(\"Sum of explained variance ratio: %d%%\" % (int(explained_variance_google * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA feature reduction - going with 100 components - tesla\n",
    "\n",
    "svd_tesla = TruncatedSVD(400)\n",
    "lsa_tesla = make_pipeline(svd_tesla, Normalizer(copy=False))\n",
    "x_train_lsa_tesla = lsa_tesla.fit_transform(train_tfidf_vectors_tesla)\n",
    "print('LSA output shape:', x_train_lsa_tesla.shape)\n",
    "explained_variance_tesla = svd_tesla.explained_variance_ratio_.sum()\n",
    "print(\"Sum of explained variance ratio: %d%%\" % (int(explained_variance_tesla * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google - pyclustering silhouette - min clusters 2, max clusters 100\n",
    "\n",
    "# Code source\n",
    "# https://codedocs.xyz/annoviko/pyclustering/classpyclustering_1_1cluster_1_1silhouette_1_1silhouette__ksearch.html\n",
    "\n",
    "search_instance = silhouette_ksearch(x_train_lsa_google,2,100, algorithm=silhouette_ksearch_type.KMEDOIDS).process()\n",
    "amount = search_instance.get_amount()\n",
    "scores = search_instance.get_scores()\n",
    "print(\"Scores: '%s'\" % str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google - Get index of max score - identify optimal # of clusters\n",
    "\n",
    "# Code source\n",
    "# https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary\n",
    "\n",
    "max(scores.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tesla - pyclustering silhouette - min clusters 2, max clusters 50\n",
    "\n",
    "search_instance = silhouette_ksearch(x_train_lsa_tesla,2,30, algorithm=silhouette_ksearch_type.KMEDOIDS).process()\n",
    "amount = search_instance.get_amount()\n",
    "scores = search_instance.get_scores()\n",
    "print(\"Scores: '%s'\" % str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tesla - Get index of max score - identify optimal # of clusters\n",
    "\n",
    "max(scores.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tolist needed for k-medoids\n",
    "google_list = df_med_google.values.tolist()\n",
    "tesla_list = df_med_tesla.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google - Kmedoids - with optimal clusters\n",
    "\n",
    "# Code source\n",
    "# https://github.com/annoviko/pyclustering/blob/master/pyclustering/cluster/ema.py\n",
    "# https://github.com/annoviko/pyclustering/issues/366\n",
    "# https://codedocs.xyz/annoviko/pyclustering/classpyclustering_1_1cluster_1_1kmedoids_1_1kmedoids.html\n",
    "# https://github.com/letiantian/kmedoids\n",
    "\n",
    "# set random initial medoids\n",
    "initial_medoids_google = list(range(0,17))\n",
    "\n",
    "# create instance of K-Medoids algorithm\n",
    "kmedoids_instance_google = kmedoids(google_list, initial_medoids_google)\n",
    "\n",
    "# run cluster analysis and obtain results\n",
    "kmedoids_instance_google.process();\n",
    "clusters_google = kmedoids_instance_google.get_clusters()\n",
    "medoids_google = kmedoids_instance_google.get_medoids();\n",
    "\n",
    "print(\"Amount of clusters - Google:\", len(clusters_google));\n",
    "for cluster in clusters_google:\n",
    "    print(\"Cluster length:\", len(cluster));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tesla - Kmedoids - with optimal clusters \n",
    "\n",
    "# set random initial medoids\n",
    "initial_medoids_tesla = list(range(0,17))\n",
    "\n",
    "# create instance of K-Medoids algorithm\n",
    "kmedoids_instance_tesla = kmedoids(tesla_list, initial_medoids_tesla)\n",
    "\n",
    "# run cluster analysis and obtain results\n",
    "kmedoids_instance_tesla.process();\n",
    "clusters_tesla = kmedoids_instance_tesla.get_clusters()\n",
    "medoids_tesla = kmedoids_instance_tesla.get_medoids();\n",
    "\n",
    "print(\"Amount of clusters - Tesla:\", len(clusters_tesla));\n",
    "for cluster in clusters_tesla:\n",
    "    print(\"Cluster length:\", len(cluster));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google - Create a file with cluster and original index of cluster\n",
    "\n",
    "# Code from Dr. George\n",
    "\n",
    "cluster_number_google = []\n",
    "indices_google = []\n",
    "for i, c in enumerate(clusters_google):\n",
    "    cluster_number_google.extend([i] * len(c))\n",
    "    indices_google.extend(c)\n",
    "\n",
    "df_google_cluster = pd.DataFrame({'cluster': cluster_number_google, 'index': indices_google})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tesla - Create a file with cluster and original index of cluster\n",
    "\n",
    "# Code from Dr. George\n",
    "\n",
    "cluster_number_tesla = []\n",
    "indices_tesla = []\n",
    "for i, c in enumerate(clusters_tesla):\n",
    "    cluster_number_tesla.extend([i] * len(c))\n",
    "    indices_tesla.extend(c)\n",
    "\n",
    "df_tesla_cluster = pd.DataFrame({'cluster': cluster_number_tesla, 'index': indices_tesla})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line up indexes and add cluster to original dataframes\n",
    "\n",
    "df_google_cluster.set_index('index', inplace=True)\n",
    "df_tesla_cluster.set_index('index', inplace=True)\n",
    "\n",
    "df_google_cluster.sort_index(inplace=True)\n",
    "df_tesla_cluster.sort_index(inplace=True)\n",
    "\n",
    "df_med_google['cluster'] = df_google_cluster['cluster']\n",
    "df_med_tesla['cluster'] = df_tesla_cluster['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_med_google.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and capture mean of each variable\n",
    "\n",
    "#Code source\n",
    "# https://stackoverflow.com/questions/30328646/python-pandas-group-by-in-group-by-and-average\n",
    "\n",
    "df_grouped_google = df_med_google.groupby(['cluster']).mean()\n",
    "df_grouped_tesla = df_med_tesla.groupby(['cluster']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No longer needed - Could not add additional features to K-Medoids\n",
    "# Drop some columns not needed for cluster analysis\n",
    "\n",
    "# df_grouped_google.drop(['rating','SA','current','former'], inplace=True, axis=1)\n",
    "# df_grouped_tesla.drop(['rating','SA','current','former'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to re-use countvectorizer, grabbing the clean reviews used earlier\n",
    "\n",
    "df_google_cluster_words = pd.DataFrame(cln_review_google)\n",
    "df_tesla_cluster_words = pd.DataFrame(cln_review_tesla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lining up the assigned cluster with each review\n",
    "\n",
    "df_google_cluster_words['cluster'] = df_google_cluster['cluster']\n",
    "df_tesla_cluster_words['cluster'] = df_tesla_cluster['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_cluster_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which google clusters have the highest total average tfidf\n",
    "df_google_summed = df_grouped_google.sum(axis=1)\n",
    "df_google_summed.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which tesla clusters have the highest total average tfidf\n",
    "df_tesla_summed = df_grouped_tesla.sum(axis=1)\n",
    "df_tesla_summed.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google top 3 clusters - filter records for top clusters into their own file\n",
    "first_google_cluster = df_google_cluster_words.loc[df_google_cluster_words['cluster'] == 4]\n",
    "second_google_cluster = df_google_cluster_words.loc[df_google_cluster_words['cluster'] == 1]\n",
    "third_google_cluster = df_google_cluster_words.loc[df_google_cluster_words['cluster'] == 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tesla top 3 clusters - filter records for top clusters into their own file\n",
    "first_tesla_cluster = df_tesla_cluster_words.loc[df_tesla_cluster_words['cluster'] == 10]\n",
    "second_tesla_cluster = df_tesla_cluster_words.loc[df_tesla_cluster_words['cluster'] == 11]\n",
    "third_tesla_cluster = df_tesla_cluster_words.loc[df_tesla_cluster_words['cluster'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the cluster variable to count words\n",
    "first_google_cluster.drop(columns = 'cluster', inplace=True, axis=1)\n",
    "second_google_cluster.drop(columns = 'cluster', inplace=True, axis=1)\n",
    "third_google_cluster.drop(columns = 'cluster', inplace=True, axis=1)\n",
    "\n",
    "first_tesla_cluster.drop(columns = 'cluster', inplace=True, axis=1)\n",
    "second_tesla_cluster.drop(columns = 'cluster', inplace=True, axis=1)\n",
    "third_tesla_cluster.drop(columns = 'cluster', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lists for countvectorizer\n",
    "first_google_cluster_list = first_google_cluster.values.tolist()\n",
    "second_google_cluster_list = second_google_cluster.values.tolist()\n",
    "third_google_cluster_list = third_google_cluster.values.tolist()\n",
    "\n",
    "first_tesla_cluster_list = first_tesla_cluster.values.tolist()\n",
    "second_tesla_cluster_list = second_tesla_cluster.values.tolist()\n",
    "third_tesla_cluster_list = third_tesla_cluster.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google cluster 4\n",
    "# Report out top 10 for 1,2,3, and 4 grams\n",
    "\n",
    "ngrams_google, ngram_counts_google = {}, {}\n",
    "\n",
    "first_google_cluster = [str(first_google_cluster_list)]\n",
    "\n",
    "print ('Cluster 4')\n",
    "\n",
    "for n in [1, 2, 3, 4]:\n",
    "    print('top 10', str(n) + '-grams:\\n')\n",
    "    ngrams_google[n], ngram_counts_google[n] = get_top_grams_google(first_google_cluster, n=n)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google cluster 1\n",
    "# Report out top 10 for 1,2,3, and 4 grams\n",
    "\n",
    "ngrams_google, ngram_counts_google = {}, {}\n",
    "\n",
    "print ('Cluster 1')\n",
    "\n",
    "second_google_cluster = [str(second_google_cluster_list)]\n",
    "\n",
    "for n in [1, 2, 3, 4]:\n",
    "    print('top 10', str(n) + '-grams:\\n')\n",
    "    ngrams_google[n], ngram_counts_google[n] = get_top_grams_google(second_google_cluster, n=n)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google cluster 16\n",
    "# Report out top 10 for 1,2,3, and 4 grams\n",
    "\n",
    "ngrams_google, ngram_counts_google = {}, {}\n",
    "\n",
    "print ('Cluster 16')\n",
    "\n",
    "third_google_cluster = [str(third_google_cluster_list)]\n",
    "\n",
    "for n in [1, 2, 3, 4]:\n",
    "    print('top 10', str(n) + '-grams:\\n')\n",
    "    ngrams_google[n], ngram_counts_google[n] = get_top_grams_google(third_google_cluster, n=n)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tesla cluster 10\n",
    "# Report out top 10 for 1,2,3, and 4 grams\n",
    "\n",
    "ngrams_tesla, ngram_counts_tesla = {}, {}\n",
    "\n",
    "first_tesla_cluster = [str(first_tesla_cluster_list)]\n",
    "\n",
    "print ('Cluster 10')\n",
    "\n",
    "for n in [1, 2, 3, 4]:\n",
    "    print('top 10', str(n) + '-grams:\\n')\n",
    "    ngrams_tesla[n], ngram_counts_tesla[n] = get_top_grams_tesla(first_tesla_cluster, n=n)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tesla cluster 11\n",
    "# Report out top 10 for 1,2,3, and 4 grams\n",
    "\n",
    "ngrams_tesla, ngram_counts_tesla = {}, {}\n",
    "\n",
    "second_tesla_cluster = [str(second_tesla_cluster_list)]\n",
    "\n",
    "print ('Cluster 11')\n",
    "\n",
    "for n in [1, 2, 3, 4]:\n",
    "    print('top 10', str(n) + '-grams:\\n')\n",
    "    ngrams_tesla[n], ngram_counts_tesla[n] = get_top_grams_tesla(second_tesla_cluster, n=n)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tesla cluster 2\n",
    "# Report out top 10 for 1,2,3, and 4 grams\n",
    "\n",
    "ngrams_tesla, ngram_counts_tesla = {}, {}\n",
    "\n",
    "third_tesla_cluster = [str(third_tesla_cluster_list)]\n",
    "\n",
    "print ('Cluster 2')\n",
    "\n",
    "for n in [1, 2, 3, 4]:\n",
    "    print('top 10', str(n) + '-grams:\\n')\n",
    "    ngrams_tesla[n], ngram_counts_tesla[n] = get_top_grams_tesla(third_tesla_cluster, n=n)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA/NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Source\n",
    "# https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_\n",
    "# nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the NMF model 1 - google\n",
    "\n",
    "n_components = 10\n",
    "n_top_words = 10\n",
    "\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(train_tfidf_vectors_google)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer_google.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the NMF model 2 - google\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(train_tfidf_vectors_google)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer_google.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the LDA model 1 - google\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(train_tfidf_vectors_google)\n",
    "\n",
    "tf_feature_names = tfidf_vectorizer_google.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the NMF model 1 - tesla\n",
    "\n",
    "n_components = 10\n",
    "n_top_words = 10\n",
    "\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(train_tfidf_vectors_tesla)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer_tesla.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the NMF model 2 - tesla\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(train_tfidf_vectors_tesla)\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer_tesla.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the LDA model 1 - tesla\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "lda.fit(train_tfidf_vectors_tesla)\n",
    "\n",
    "tf_feature_names = tfidf_vectorizer_tesla.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
